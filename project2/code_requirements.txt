Some of the important degrees of
freedom are:
1.  The number of hidden layers.
2.  The number of nodes in each hidden layer.  Different layers will typically have different sizes.
3.  The activation functions used in the hidden layers and output layer.
4.  The learning rate.
5.  The error function (a.k.a.  loss function) for backpropagation.
6.  The initial range of values for network weights.

2.3  The Training and Testing Scheme
This (very standard) scheme consists of several steps:
1.  Separate the data into training cases, validation cases and test cases, where all cases are assumed to consist of features and labels, i.e., the correct classification.
2.  Repeatedly pass the training features through the ANN to produce an output value, which yields an error term when compared to the correct classification.
3.  Use error terms as the basis of backpropagation to modify weights in the network,  thus learning to correctly classify the training cases.
4.  Intermittently during training, perform a validation test by turning backpropagation learning off and running the complete set of validation cases through the network one time while recording the average error over those cases.
5.  When the total training error has been sufficiently reduced by learning, turn backpropagation off.
6.  Run each test case through the ANN one time. Record the total error on the test cases and use that as an indicator of the trained ANNs ability to generalize to handle new cases (i.e., those that it has not explicitly trained on)

2.4  Visualization
As described below in more detail, your system will need to provide the following behavioral data in graphic
form:
1. A plot of progression of training-set and validation-set error (as a function of the training steps, where each step involves the processing of one minibatch).
2. A display of the weights and biases for any user-chosen areas of the network, for example, the weights between layers 1 and 2 and the biases for layer 2.  Typically, these are only shown at the end of the run, but displaying them intermittently is also useful (though not mandatory for this assignment).
3. The sets of corresponding activation levels for user-chosen layers that result from a post-training mapping run (as described below).
4. Dendrograms (also described below), which graphically display relationships between input patterns and the hidden-layer activations that they invoke.

2.6 Scenario parameters
1.  Network Dimensions - the number of layers in the network along with the size of each layer.
2.  Hidden Activation Function - that function to be used for all hidden layers (i.e., all layers except the input and output).
3.  Output  Activation  Function  -  this  is  often  different  from  the  hidden-layer  activation  function;  for example, it is common to use softmax for classification problems, but only in the final layer.
4.  Cost Function - (a.k.a.  loss function) defines the quantity to be minimized, such as mean-squared error or cross-entropy.
5.  Learning Rate - the same rate can be applied to each set of weights and biases throughout the network and throughout the training phase.  More complex schemes, for example those that reduce the learning rate throughout training, are possible but not required.
6.  Initial  Weight  Range  -  two  real  numbers,  an  upper  and  lower  bound,  to  be  used  when  randomly initializing all weights (including those from bias nodes) in the network. Optionally, this parameter may take a value such as the word scaled, indicating that the weight range will be calculated dynamically, based on the total number of neurons in the upstream layer of each weight matrix.
7. Data Source - specfied as either:
• a data file along with any functions needed to read that particular file.
• a function name (such as one of those in tflowtools.py) along with the parameters to that function, such as the number and length of data vectors to be generated, the density range (i.e.  upper and lower bound on the fraction of 1’s in the feature vectors), etc.
8. Case Fraction - Some data sources (such as MNIST) are very large, so it makes sense to only use a fraction of the total set for the combination of training, validation and testing.  This should default to 1.0, but much lower values can come in handy for huge data files.
9. Validation Fraction - the fraction of data cases to be used for validation testing.
10. Test Fraction - the fraction of the data cases to be used for standard testing.
11. Minibatch Size - the number of training cases in a minibatch
12. Map Batch Size - the number of training cases to be used for map  test (described below).  A value of zero indicates that no map test will be performed.
13. Steps - the total number of minibatches to be run through the system during training.
14. Map Layers - the layers to be visualized during the map test.
15. Map Dendrograms - list of the layers whose activation patterns (during the map test) will be used toproduce dendrograms, one per specified layer.  See below for more details on dendrograms.
16. Display Weights - list of the weight arrays to be visualized at the end of the run.
17. Display Biases - list of the bias vectors to be visualized at the end of the run

The mandatory information generated by each run of your system is:
1. A plot of the progression of the training-set error from start to end of training.  Each data point is the average (per case) error for a single mini-batch.
2. A plot of the progression of the validation-set error from start to end of training.
3. A listing of the error percentage for the test set, as evaluated after training has finished.

your  code  will  need  to  perform  the  following  sequence  of activities:
1. Train the network to a desired level of expertise (or lack thereof).  You need not save any data during this training phase but will surely want to monitor the error progression.
2. Declare grab variables as any Tensorflow variables that you wish to monitor during the post-training phase.
3.  Determine the cases that the network will process in the post-training phase.  These may or may not be the same as the training cases.
4.  Run the network in mapping mode, which involves only one forward-pass per case.  This can either be a test mode in which the errors of each case are calculated and summed or a simple prediction mode in which the output values of the network are the final objective.  Either way, you must insure that a) no learning occurs, and b) the values of monitored (grabbed) variables are gathered and stored by your code (after each call to session.run). For these post-training situations, one call to session.run should be sufficient, with a map batch containing all the relevant cases.
5.  When  the  mapping  run  finishes,  the  gathered  rows  of  data  (one  row  per  neural-net  layer  per  case) are sent (normally as a 2-d array) to a plotting routine, such as hinton plot or display matrix in tflowtools.p

You will probably want to define a new method called do mapping, which will behave similarly to method do testing in tutor3.py, although it need not have self.error as its main operator,  since self.predictor would suffice.  It will also need code for gathering and storing the grabbed values.  Be aware that the resulting dimensions of the grabbed variables could vary depending upon whether you run all the cases through as a single mini-batch or whether you perform N calls to session.run, where N is the number of cases
